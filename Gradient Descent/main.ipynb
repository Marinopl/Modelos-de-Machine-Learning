{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "631d9185",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "* É um **algoritmo iterativo** utilizado para encontrar o mínimo de uma função diferenciável.\n",
    "\n",
    "* Em Machine Learning, utiliza-se este algoritmo para minimizar a função custo (cost function) como em Regressão Linear e encontrar parâmetros otimizados.\n",
    "\n",
    "* Uma **função custo** é a função que mede a variação (desvio) da predição do modelo do valor real, minimizando os erros.\n",
    "\n",
    "* Este algoritmo é baseado em uma função convexa. De início, escolhe-se um ponto arbitrário da base de dados para encontrar sua performance. Então, calcula-se a derivada desta função para encontrar a tangente da função modelada. A inclinação da tangente informa o que deve modificado nos parametros (bias). A ideia é que, incialmente, a inclinação deve ser grosseira, mas ir reduzindo conforme o algoritmo é otimizado até atingir o mínimo desejável.\n",
    "\n",
    "    - **Taxa de aprendizagem (Learning Rate)** é a quantidade de passos até chegar ao minímo da função custo. Ele é calculado e modificado conforme a função custo analisada. Quanto menor, mais precisão, porém muitas iterações são necessárias para chegar ao final do cálculo.\n",
    "\n",
    "* Como exemplo, considere dois parâmetros de controle: $\\theta_0$ e $\\theta_1$ e uma taxa de aprendizagem $\\alpha$. O processo iterativo para modificar $\\alpha$ é:\n",
    "$$\\theta_0 \\to \\theta_0 - \\alpha \\frac{\\partial J(\\theta_0, \\theta_1)}{\\partial \\theta_0} \\ \\ e \\ \\ \\theta_1 \\to \\theta_1 - \\alpha \\frac{\\partial J(\\theta_0, \\theta_1)}{\\partial \\theta_1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac54efb",
   "metadata": {},
   "source": [
    "## Tipos de Gradiente Descent\n",
    "\n",
    "### Batch gradient descent:\n",
    "* Este modelos considera a soma dos erros para cada ponto do conjunto de treinamento, atualizando o modelo apenas após todos o conjunto de treinamento ser calculado. Apesar de ser eficiente no sentido de menos passos, leva um tempo considerável para grandes datasets e necessita de armazenamento em memória compatível com o tamanho do dataset. Não é o mais ideal.\n",
    "\n",
    "### Stochastic gradient descent:\n",
    "* Executa uma iteração para cada dado dentro do conjunto de dados de treinamento e atualiza os parâmetros do conjunto a cada iteração. Como só é necessário armazenar um único conjunto de treinamento na mémora, eles tem baixo custo de armazenamento. Essas atualizações garantem mais velocidade e otimização aos dados, mas leva mais tempo do que o normal, porém melhor que o Batch.\n",
    "\n",
    "* Este tipo de execução resultam em gradientes com ruídos, mas também podem ser úteis para encontrar minímos globais ao invés de locais. \n",
    "\n",
    "### Mini-batch gradient descent:\n",
    "* É a junção dos dois modelos. Ele divide o conjunto de treinamento em subconjuntos menores e performa diversos batchs a cada iteração."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e50e18",
   "metadata": {},
   "source": [
    "## Exemplo\n",
    "\n",
    "* Vamos considerar a função $f(x) = 3w_0^2 + 4w_1^2 - 5w_0 + 7$ e encontrar o mínimo dela pelo algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "286f095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d08ce59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5 10]\n",
      "457\n",
      "[4.25 7.6 ]\n",
      "270.97749999999996\n",
      "[3.635 5.776]\n",
      "161.91337899999996\n",
      "[3.1307  4.38976]\n",
      "97.83031890039999\n",
      "[2.717174  3.3362176]\n",
      "60.08462513702703\n",
      "[2.37808268 2.53552538]\n",
      "37.790974028107705\n",
      "[2.1000278  1.92699929]\n",
      "24.583516253356322\n",
      "[1.87202279 1.46451946]\n",
      "16.732563015773216\n",
      "[1.68505869 1.11303479]\n",
      "12.048360674226597\n",
      "[1.53174813 0.84590644]\n",
      "9.242247148028154\n",
      "[1.40603346 0.64288889]\n",
      "7.553847501493853\n",
      "[1.30294744 0.48859556]\n",
      "6.533181375871123\n",
      "[1.2184169  0.37133262]\n",
      "5.9130864019803235\n",
      "[1.14910186 0.28221279]\n",
      "5.534372198113799\n",
      "[1.09226352 0.21448172]\n",
      "5.301810837873198\n",
      "[1.04565609 0.16300611]\n",
      "5.158193493533129\n",
      "[1.00743799 0.12388464]\n",
      "5.068993584890293\n",
      "[0.97609915 0.09415233]\n",
      "5.013271550515018\n",
      "[0.95040131 0.07155577]\n",
      "4.978262311072791\n",
      "[0.92932907 0.05438239]\n",
      "4.956141987409894\n",
      "[0.91204984 0.04133061]\n",
      "4.9420884096295845\n",
      "[0.89788087 0.03141127]\n",
      "4.933112489776624\n",
      "[0.88626231 0.02387256]\n",
      "4.927350693764443\n",
      "[0.8767351  0.01814315]\n",
      "4.9236345007040905\n",
      "[0.86892278 0.01378879]\n",
      "4.9212270155731\n",
      "[0.86251668 0.01047948]\n",
      "4.919660947679641\n",
      "[0.85726368 0.00796441]\n",
      "4.918638377650819\n",
      "[0.85295621 0.00605295]\n",
      "4.917968391806973\n",
      "[0.8494241  0.00460024]\n",
      "4.917528053450235\n",
      "[0.84652776 0.00349618]\n",
      "4.917237838427171\n",
      "[0.84415276 0.0026571 ]\n",
      "4.917046087484336\n",
      "[0.84220526 0.0020194 ]\n",
      "4.9169191120056706\n",
      "[0.84060832 0.00153474]\n",
      "4.916834864551037\n",
      "[0.83929882 0.0011664 ]\n",
      "4.91677886974567\n",
      "[0.83822503 0.00088647]\n",
      "4.9167415961171175\n",
      "[8.37344527e-01 6.73714244e-04]\n",
      "4.916716751245383\n"
     ]
    }
   ],
   "source": [
    "def f(w):\n",
    "    return 3*w[0]**2 + 4*w[1]**2 - 5*w[0] + 7\n",
    "\n",
    "def grad(w):\n",
    "    df_dw0 = 6*w[0] - 5\n",
    "    df_dw1 = 8*w[1]\n",
    "    return np.array([df_dw0, df_dw1])\n",
    "\n",
    "def gradient_descent(w_new, w_prev, lr):\n",
    "    print(w_prev)\n",
    "    print(f(w_prev))\n",
    "    while True:\n",
    "        w_prev = w_new\n",
    "        w_0 = w_prev[0] - lr*grad(w_prev)[0]\n",
    "        w_1 = w_prev[1] - lr*grad(w_prev)[1]\n",
    "        w_new = np.array([w_0, w_1])\n",
    "        print(w_new)\n",
    "        print(f(w_new))\n",
    "        if (w_new[0] - w_prev[0])**2 + (w_new[1] - w_prev[1])**2 < 1e-6:\n",
    "            break\n",
    "\n",
    "gradient_descent(np.array([5, 10]), np.array([5, 10]), 0.03)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
